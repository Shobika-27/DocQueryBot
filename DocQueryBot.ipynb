{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocuQueryBot\n",
    "\n",
    "A chatbot application that allows users to upload PDF documents and ask questions based on the document's content.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to build a chatbot that:\n",
    "- Processes uploaded PDF documents.\n",
    "- Splits the text into chunks and generates embeddings.\n",
    "- Uses a conversational chain to retrieve answers from the document.\n",
    "- Provides an interactive Gradio-based user interface.\n",
    "\n",
    "The chatbot ensures that:\n",
    "1. Answers are strictly based on the document content.\n",
    "2. If the requested information is unavailable, it explicitly states that.\n",
    "3. No assumptions or fabricated responses are generated.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "# LangChain for document processing, Chroma for vector storage, and Hugging Face for embeddings and LLM.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device for computation (CPU/GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial System Message\n",
    "# This sets the tone for the chatbot, ensuring it answers only based on the document's content.\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"Provide answers strictly based on the document content. If the requested information is not found in the document, respond that the information is not available in the document. Do not generate or assume answers outside of the document's contents. Answer only with information directly found in the document.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process the document\n",
    "def load_and_embed_document(file_path, persist_directory=\"docs/chroma\"):\n",
    "    # Load the document\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    print(\"PDF text extracted successfully.\")\n",
    "\n",
    "    # Split the document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Set up embeddings with HuggingFace\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Initialize Chroma vector store and add documents\n",
    "    vectordb = Chroma.from_documents(docs, embedding, persist_directory=persist_directory)\n",
    "    print(\"Chroma collection created and embeddings stored.\")\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "# Function to create the conversational chain\n",
    "def create_conversational_chain(vectordb):\n",
    "    # Load the model and tokenizer\n",
    "    model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token if missing\n",
    "    # Set up the Hugging Face model for response generation\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id, \n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32,\n",
    "        max_new_tokens=150,  \n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        )\n",
    "    # Wrap the pipeline with HuggingFacePipeline for LangChain compatibility\n",
    "    llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "    # Define the conversational retrieval chain\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", \n",
    "        return_messages=True,\n",
    "        output_key=\"answer\",\n",
    "        )\n",
    "    \n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm( \n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=False,\n",
    "        output_key=\"answer\",\n",
    "        )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "# Gradio UI setup\n",
    "def chatbot_interface(file, user_input, chat_history=[]):\n",
    "    # Load the document and initialize vector store\n",
    "    vectordb = load_and_embed_document(file.name)\n",
    "    qa_chain = create_conversational_chain(vectordb)\n",
    "    \n",
    "    \n",
    "    # Get the response from the conversational chain\n",
    "    response = qa_chain({\"question\": user_input, \"chat_history\": chat_history})     \n",
    "\n",
    "    # Extract only the answer text using regex, ignoring metadata or context instructions\n",
    "    full_answer = response.get(\"answer\", \"\").strip()\n",
    "\n",
    "    # Remove everything up to and including \"Helpful Answer:\" to show only the clean response\n",
    "    if \"Helpful Answer:\" in full_answer:\n",
    "        answer_start = full_answer.index(\"Helpful Answer:\") + len(\"Helpful Answer:\")\n",
    "        clean_answer = full_answer[answer_start:].strip()\n",
    "    else:\n",
    "        clean_answer = full_answer  \n",
    "\n",
    "    chat_history.append((user_input, clean_answer))\n",
    "\n",
    "    return chat_history, chat_history\n",
    "\n",
    "# Gradio app setup\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Document-based Chatbot\")\n",
    "    with gr.Row():\n",
    "        file = gr.File(label=\"Upload a PDF Document\")\n",
    "    user_input = gr.Textbox(label=\"Enter your question\")\n",
    "    chat_history = gr.State([])  # Store chat history\n",
    "\n",
    "    # Display chat messages\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input.submit(chatbot_interface, inputs=[file, user_input, chat_history], outputs=[chatbot, chat_history])\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
